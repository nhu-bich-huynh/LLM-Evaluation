{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Europarl Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oskar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from google import genai\n",
    "import Levenshtein\n",
    "from random_word import RandomWords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dataset\n",
    "# def load_data(danish_file, english_file):\n",
    "#     with open(danish_file, \"r\", encoding=\"utf-8\") as f_da, open(english_file, \"r\", encoding=\"utf-8\") as f_en:\n",
    "#         danish_sentences = [line.strip() for line in f_da.readlines()]\n",
    "#         english_sentences = [line.strip() for line in f_en.readlines()]\n",
    "#     return danish_sentences, english_sentences\n",
    "\n",
    "# danish_sentences, english_sentences = load_data(\"../da.da\", \"../en.en\")\n",
    "\n",
    "# # Tokenization\n",
    "# danish_tokens = [word_tokenize(sent.lower()) for sent in danish_sentences]\n",
    "# english_tokens = [word_tokenize(sent.lower()) for sent in english_sentences]\n",
    "\n",
    "# # Flatten lists for word statistics\n",
    "# danish_words = [word for sent in danish_tokens for word in sent]\n",
    "# english_words = [word for sent in english_tokens for word in sent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_sentences_da = len(danish_sentences)\n",
    "# num_sentences_en = len(english_sentences)\n",
    "\n",
    "# num_words_da = len(danish_words)\n",
    "# num_words_en = len(english_words)\n",
    "\n",
    "# unique_words_da = len(set(danish_words))\n",
    "# unique_words_en = len(set(english_words))\n",
    "\n",
    "# ttr_da = unique_words_da / num_words_da * 100\n",
    "# ttr_en = unique_words_en / num_words_en * 100\n",
    "\n",
    "# avg_length_da = np.mean([len(sent) for sent in danish_tokens])\n",
    "# avg_length_en = np.mean([len(sent) for sent in english_tokens])\n",
    "\n",
    "# std_length_da = np.std([len(sent) for sent in danish_tokens])\n",
    "# std_length_en = np.std([len(sent) for sent in english_tokens])\n",
    "\n",
    "# min_length_da = np.min([len(sent) for sent in danish_tokens])\n",
    "# min_length_en = np.min([len(sent) for sent in english_tokens])\n",
    "\n",
    "# max_length_da = np.max([len(sent) for sent in danish_tokens])\n",
    "# max_length_en = np.max([len(sent) for sent in english_tokens])\n",
    "\n",
    "# sentence_length_ratio = avg_length_da / avg_length_en \n",
    "\n",
    "# print(f\"Total Sentences: Danish = {num_sentences_da}, English = {num_sentences_en}\")\n",
    "# print(f\"Total Words: Danish = {num_words_da}, English = {num_words_en}\")\n",
    "# print(f\"Unique Words: Danish = {unique_words_da}, English = {unique_words_en}\")\n",
    "# print(f\"Type-Token Ratio: Danish = {ttr_da:.2f}%, English = {ttr_en:.2f}%\")\n",
    "# print(f\"Avg. Sentence Length: Danish = {avg_length_da:.2f}, English = {avg_length_en:.2f}\")\n",
    "# print(f\"Std Dev Sentence Length: Danish = {std_length_da:.2f}, English = {std_length_en:.2f}\")\n",
    "# print(f\"Sentence Length Ratio (DA/EN): {sentence_length_ratio:.2f}\")\n",
    "# print(f\"Min sentence length (DA/EN): Danish = {min_length_da}, English = {min_length_en}\")\n",
    "# print(f\"Max sentence length (DA/EN): Danish = {max_length_da}, English = {max_length_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"English\"\n",
    "\n",
    "# with open(\"../en.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(200):\n",
    "#         lines = [next(f).strip() for _ in range(10000)]\n",
    "#         df = pandas.DataFrame({\"Text\": lines})\n",
    "#         output_file = f\"dataframes/en/{name}{i+1}.csv\"\n",
    "#         df.to_csv(output_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# name = \"Danish\"\n",
    "\n",
    "# with open(\"../da.da\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(200):\n",
    "#         lines = [next(f).strip() for _ in range(10000)]\n",
    "#         df = pandas.DataFrame({\"Text\": lines})\n",
    "#         output_file = f\"dataframes/da/{name}{i+1}.csv\"\n",
    "#         df.to_csv(output_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "\n",
    "# danishData = pandas.read_fwf(\"test.txt\", header=None, names=[\"test_sentences\"])\n",
    "# englishData = pandas.read_fwf(\"test.txt\", header=None, names=[\"test_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies (First, look at README)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Oskar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Oskar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "r = RandomWords()\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    reference_p = [word_tokenize(reference)]\n",
    "    candidate_p = word_tokenize(candidate)\n",
    "    score = sentence_bleu(reference_p, candidate_p)\n",
    "    return score\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "  reference = word_tokenize(reference)\n",
    "  candidate = word_tokenize(candidate)\n",
    "  meteor_score = round(meteor([candidate],reference), 4)\n",
    "  return meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Rouge test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 0.25, 'p': 0.25, 'f': 0.24999999500000009}, 'rouge-l': {'r': 0.8, 'p': 0.8, 'f': 0.7999999950000002}}]\n"
     ]
    }
   ],
   "source": [
    "sen1 = \"Hello, what are you doing?\"\n",
    "sen2 = \"Hello, what you are doing?\"\n",
    "res = calculate_rouge(sen1, sen2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform BLEU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.86809206056511e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "print(calculate_bleu(sen1, sen2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform METEOR test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9067\n"
     ]
    }
   ],
   "source": [
    "print(calculate_meteor(sen1, sen2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace with synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findSynonym(w):\n",
    "#     res = []\n",
    "#     for x in wordnet.synsets(w):\n",
    "#         for y in x.lemma_names():\n",
    "#             res.append(y.replace(\"_\", \" \"))\n",
    "#     if len(res) > 0:\n",
    "#         word = random.choice(list(set(res)))\n",
    "#         return [not word==w, word]\n",
    "#     else: return [False, \"\"] \n",
    "\n",
    "# def replaceWordWithSynonym(sen):\n",
    "#     toks = word_tokenize(sen.lower())\n",
    "#     indx = random.randrange(len(toks))\n",
    "#     syn = findSynonym(toks[indx])\n",
    "#     if syn[0]:\n",
    "#         res = toks\n",
    "#         res[indx] = syn[1]\n",
    "#         return True, TreebankWordDetokenizer().detokenize(res)\n",
    "#     else: return False, TreebankWordDetokenizer().detokenize(toks)\n",
    "\n",
    "# print(replaceWordWithSynonym(\"Hello my name is Oskar and I am 28 years old\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace word with antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findAntonym(w):\n",
    "#     res = []\n",
    "#     for x in wordnet.synsets(w):\n",
    "#         for y in x.lemma_names():\n",
    "#             res.append(y.replace(\"_\", \" \"))\n",
    "#     if len(res) > 0:\n",
    "#         word = random.choice(list(set(res)))\n",
    "#         return [not word==w, word]\n",
    "#     else: return [False, \"\"] \n",
    "\n",
    "# def replaceWordWithAntonym(sen):\n",
    "#     toks = word_tokenize(sen.lower())\n",
    "#     indx = random.randrange(len(toks))\n",
    "#     syn = findSynonym(toks[indx])\n",
    "#     if syn[0]:\n",
    "#         res = toks\n",
    "#         res[indx] = syn[1]\n",
    "#         return True, TreebankWordDetokenizer().detokenize(res)\n",
    "#     else: return False, TreebankWordDetokenizer().detokenize(toks)\n",
    "\n",
    "# print(replaceWordWithSynonym(\"Hello my name is Oskar and I am 28 years old\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace token with random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, '.')\n"
     ]
    }
   ],
   "source": [
    "def replaceWithRandomWord(sen):\n",
    "    randomWord = r.get_random_word()\n",
    "    toks = word_tokenize(sen.lower())\n",
    "    if len(toks) < 2:\n",
    "        return False, sen\n",
    "    indx = random.randrange(len(toks))\n",
    "    toks[indx] = randomWord\n",
    "    res = TreebankWordDetokenizer().detokenize(toks)\n",
    "    return (not res==sen.lower()), res\n",
    "\n",
    "print(replaceWithRandomWord(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swap Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'what hello, are you doing? ')\n"
     ]
    }
   ],
   "source": [
    "def swapWords(sen):\n",
    "    words = sen.count(\" \") + 1\n",
    "    if words < 2:\n",
    "        return False, sen\n",
    "    toSwap = random.randint(0, words-2)\n",
    "    wordList = sen.split()\n",
    "    tmp = wordList[toSwap]\n",
    "    wordList[toSwap] = wordList[toSwap+1]\n",
    "    wordList[toSwap+1] = tmp\n",
    "    res = \"\"\n",
    "    for x in wordList:\n",
    "        res += x + \" \"\n",
    "    return (not res.lower()==sen.lower()), res.lower()\n",
    "\n",
    "print(swapWords(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 'Hello.')\n"
     ]
    }
   ],
   "source": [
    "def removeToken(sen):\n",
    "    toks = word_tokenize(sen.lower())\n",
    "    if len(toks) < 3:\n",
    "        return False, sen\n",
    "    indx = random.randrange(len(toks))\n",
    "    del toks[indx]\n",
    "    res = TreebankWordDetokenizer().detokenize(toks)\n",
    "    return (not res==sen.lower()), res\n",
    "\n",
    "print(removeToken(\"Hello.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add random word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'rancescent hello, what are you doing?')\n"
     ]
    }
   ],
   "source": [
    "def addRandomWord(sen):\n",
    "    if len(sen) < 1:\n",
    "        return False, sen\n",
    "    toks = word_tokenize(sen.lower())\n",
    "    indx = random.randrange(len(toks))\n",
    "    toks.insert(indx, r.get_random_word())\n",
    "    res = TreebankWordDetokenizer().detokenize(toks)\n",
    "    return (not res==sen.lower()), res\n",
    "\n",
    "print(addRandomWord(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'hello, what are you you doing?')\n"
     ]
    }
   ],
   "source": [
    "def duplicateToken(sen):\n",
    "    if len(sen) < 1:\n",
    "        return False, sen\n",
    "    toks = word_tokenize(sen.lower())\n",
    "    indx = random.randrange(len(toks))\n",
    "    word = toks[indx]\n",
    "    toks.insert(indx, word)\n",
    "    res = TreebankWordDetokenizer().detokenize(toks)\n",
    "    return (not res==sen.lower()), res\n",
    "\n",
    "print(duplicateToken(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'bello, what are you doing?')\n"
     ]
    }
   ],
   "source": [
    "def replaceCharacter(sen):\n",
    "    if len(sen) < 1:\n",
    "        return False, sen\n",
    "    lst = list(sen)\n",
    "    indx = random.randrange(len(lst))\n",
    "    lst[indx] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "    res = \"\".join(lst)\n",
    "    return (not sen == res), res\n",
    "\n",
    "print(replaceCharacter(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'Hello, what are you doinng?')\n"
     ]
    }
   ],
   "source": [
    "def duplicateCharacter(sen):\n",
    "    if len(sen) < 1:\n",
    "        return False, sen\n",
    "    lst = list(sen)\n",
    "    indx = random.randrange(len(lst))\n",
    "    lst.insert(indx, lst[indx])\n",
    "    res = \"\".join(lst)\n",
    "    return (not res==sen), res\n",
    "\n",
    "print(duplicateCharacter(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'Hello, wht are you doing?')\n"
     ]
    }
   ],
   "source": [
    "def removeCharacter(sen):\n",
    "    if len(sen) < 2:\n",
    "        return False, sen\n",
    "    lst = list(sen)\n",
    "    indx = random.randrange(len(lst))\n",
    "    del lst[indx]\n",
    "    res = \"\".join(lst)\n",
    "    return (not res==sen), res\n",
    "\n",
    "print(removeCharacter(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'Hello, what are yo udoing?')\n"
     ]
    }
   ],
   "source": [
    "def swapCharacters(sen):\n",
    "    if len(sen) < 2:\n",
    "        return False, sen\n",
    "    lst = list(sen)\n",
    "    indx = random.randrange(len(lst)-1)\n",
    "    temp = lst[indx]\n",
    "    lst[indx] = lst[indx+1]\n",
    "    lst[indx+1] = temp\n",
    "    res = \"\".join(lst)\n",
    "    return (not res==sen), res\n",
    "\n",
    "print(swapCharacters(\"Hello, what are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE/RESET OUTPUT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetTests():\n",
    "    name = \"output.csv\"\n",
    "    cols = [\"originalSentence\", \"danishSentence\", \"tamperingType\", \"tamperedSentence\", \"LLMScore\", \"BLEU\", \"METEOR\", \"Rouge1 r\", \"Rouge1 p\", \"Rouge1 f\", \"Rouge2 r\", \"Rouge2 p\", \"Rouge2 f\", \"Rougel r\", \"Rougel p\", \"Rougel f\"]\n",
    "\n",
    "    with open(name, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=cols)\n",
    "        writer.writeheader()\n",
    "\n",
    "resetTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Oskar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm: addLLMScore(\u001b[33m\"\u001b[39m\u001b[33moutput.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m#Test(\"en_test_data.csv\", \"da_test_data.csv\", True)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mTest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataframes/en/English1.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataframes/da/Danish1.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mTest\u001b[39m\u001b[34m(enFile, daFile, llm)\u001b[39m\n\u001b[32m     61\u001b[39m         time.sleep(\u001b[32m4\u001b[39m)\n\u001b[32m     62\u001b[39m     data.to_csv(\u001b[33m\"\u001b[39m\u001b[33moutput.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mtestDataSets\u001b[49m\u001b[43m(\u001b[49m\u001b[43menFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m llm: addLLMScore(\u001b[33m\"\u001b[39m\u001b[33moutput.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mTest.<locals>.testDataSets\u001b[39m\u001b[34m(en, da)\u001b[39m\n\u001b[32m     32\u001b[39m original = row[\u001b[33m\"\u001b[39m\u001b[33mText\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     33\u001b[39m transRef = daSens.iloc[index][\u001b[33m\"\u001b[39m\u001b[33mText\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtestAllTamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransRef\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mTest.<locals>.testAllTamps\u001b[39m\u001b[34m(sen, da)\u001b[39m\n\u001b[32m     22\u001b[39m runTest(sen, duplicateToken, \u001b[33m\"\u001b[39m\u001b[33mDuplicate Token\u001b[39m\u001b[33m\"\u001b[39m, da)\n\u001b[32m     23\u001b[39m runTest(sen, replaceCharacter, \u001b[33m\"\u001b[39m\u001b[33mReplace Character\u001b[39m\u001b[33m\"\u001b[39m, da)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mrunTest\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduplicateCharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDuplicate character\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m runTest(sen, removeCharacter, \u001b[33m\"\u001b[39m\u001b[33mRemove Character\u001b[39m\u001b[33m\"\u001b[39m ,da)\n\u001b[32m     26\u001b[39m runTest(sen, swapCharacters, \u001b[33m\"\u001b[39m\u001b[33mSwap characters\u001b[39m\u001b[33m\"\u001b[39m, da)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mTest.<locals>.runTest\u001b[39m\u001b[34m(sen, tamp, tampType, da)\u001b[39m\n\u001b[32m      8\u001b[39m rouge = calculate_rouge(sen, tampSen[\u001b[32m1\u001b[39m])\n\u001b[32m      9\u001b[39m res = {\u001b[33m\"\u001b[39m\u001b[33moriginalSentence\u001b[39m\u001b[33m\"\u001b[39m: sen, \u001b[33m\"\u001b[39m\u001b[33mdanishSentence\u001b[39m\u001b[33m\"\u001b[39m: da, \u001b[33m\"\u001b[39m\u001b[33mtamperingType\u001b[39m\u001b[33m\"\u001b[39m: tampType, \u001b[33m\"\u001b[39m\u001b[33mtamperedSentence\u001b[39m\u001b[33m\"\u001b[39m: tampSen[\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mLLMScore\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBLEU\u001b[39m\u001b[33m\"\u001b[39m: calculate_bleu(sen, tampSen[\u001b[32m1\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33mMETEOR\u001b[39m\u001b[33m\"\u001b[39m: calculate_meteor(sen, tampSen[\u001b[32m1\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33mRouge1 r\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-1\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRouge1 p\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-1\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mp\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRouge1 f\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-1\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRouge2 r\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-2\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRouge2 p\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-2\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mp\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRouge2 f\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-2\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRougel r\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-l\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRougel p\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-l\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mp\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRougel f\u001b[39m\u001b[33m\"\u001b[39m: rouge[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrouge-l\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     11\u001b[39m     writer = csv.DictWriter(file, fieldnames=columns)\n\u001b[32m     13\u001b[39m     writer.writerow(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:186\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def Test(enFile, daFile, llm):\n",
    "    def runTest(sen, tamp, tampType, da):\n",
    "        # Runs all testing metrics except LLM and adds results to output file\n",
    "        outputName = \"output.csv\"\n",
    "        columns = [\"originalSentence\", \"danishSentence\", \"tamperingType\", \"tamperedSentence\", \"LLMScore\", \"BLEU\", \"METEOR\", \"Rouge1 r\", \"Rouge1 p\", \"Rouge1 f\", \"Rouge2 r\", \"Rouge2 p\", \"Rouge2 f\", \"Rougel r\", \"Rougel p\", \"Rougel f\"]\n",
    "        tampSen = tamp(sen)\n",
    "        if tampSen[0]:\n",
    "            rouge = calculate_rouge(sen, tampSen[1])\n",
    "            res = {\"originalSentence\": sen, \"danishSentence\": da, \"tamperingType\": tampType, \"tamperedSentence\": tampSen[1], \"LLMScore\": -1, \"BLEU\": calculate_bleu(sen, tampSen[1]), \"METEOR\": calculate_meteor(sen, tampSen[1]), \"Rouge1 r\": rouge[0][\"rouge-1\"]['r'], \"Rouge1 p\": rouge[0][\"rouge-1\"]['p'], \"Rouge1 f\": rouge[0][\"rouge-1\"]['f'], \"Rouge2 r\": rouge[0][\"rouge-2\"]['r'], \"Rouge2 p\": rouge[0][\"rouge-2\"]['p'], \"Rouge2 f\": rouge[0][\"rouge-2\"]['f'], \"Rougel r\": rouge[0][\"rouge-l\"]['r'], \"Rougel p\": rouge[0][\"rouge-l\"]['p'], \"Rougel f\": rouge[0][\"rouge-l\"]['f']}\n",
    "            with open(outputName, mode=\"a\", newline=\"\", encoding='utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=columns)\n",
    "                \n",
    "                writer.writerow(res)\n",
    "\n",
    "\n",
    "\n",
    "    def testAllTamps(sen, da):\n",
    "        runTest(sen, replaceWithRandomWord, \"Replace Token with Random Word\", da)\n",
    "        runTest(sen, swapWords, \"Swap words\", da)\n",
    "        runTest(sen, removeToken, \"Remove Token\", da)\n",
    "        runTest(sen, addRandomWord, \"Add random word\", da)\n",
    "        runTest(sen, duplicateToken, \"Duplicate Token\", da)\n",
    "        runTest(sen, replaceCharacter, \"Replace Character\", da)\n",
    "        runTest(sen, duplicateCharacter, \"Duplicate character\", da)\n",
    "        runTest(sen, removeCharacter, \"Remove Character\" ,da)\n",
    "        runTest(sen, swapCharacters, \"Swap characters\", da)\n",
    "\n",
    "    def testDataSets(en, da):\n",
    "        enSens = pandas.read_csv(en, encoding='utf-8')\n",
    "        daSens = pandas.read_csv(da, encoding='utf-8')\n",
    "        for index, row in enSens.iterrows():\n",
    "            original = row[\"Text\"]\n",
    "            transRef = daSens.iloc[index][\"Text\"]\n",
    "            testAllTamps(original, transRef)\n",
    "\n",
    "    def makePrompt(dan, en, tamp):\n",
    "        res = f\"\"\"Here are two sentences perfectly translated from Danish into English:\n",
    "\n",
    "        Sentence 1: {dan}\n",
    "        Sentence 2: {en}\n",
    "\n",
    "    Now I will give you a third sentence, which is another translation in English. Compare the similarity between sentence 2 and 3 on a scale from 0 (totally different) to 1 (totally identical). Return ONLY that number.\n",
    "\n",
    "        Sentence 3: {tamp}\"\"\"\n",
    "\n",
    "        return res\n",
    "\n",
    "    def addLLMScore(res):\n",
    "        client = genai.Client(api_key=\"AIzaSyBEx6cNuDD9XgrV0oO10TZ7ZQzzddCr_r8\")\n",
    "        data = pandas.read_csv(res, encoding='utf-8')\n",
    "        for index, row in data.iterrows():\n",
    "            english = row[\"originalSentence\"]\n",
    "            danish = row[\"danishSentence\"]\n",
    "            tamp = row[\"tamperedSentence\"]\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\", contents=makePrompt(danish, english, tamp)\n",
    "            )\n",
    "            llmRes = response.text\n",
    "            llmResClean = llmRes.replace(\"\\n\",\"\")\n",
    "            data.at[index, \"LLMScore\"] = llmResClean\n",
    "            time.sleep(4)\n",
    "        data.to_csv(\"output.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "    testDataSets(enFile, daFile)\n",
    "    if llm: addLLMScore(\"output.csv\")\n",
    "\n",
    "#Test(\"en_test_data.csv\", \"da_test_data.csv\", True)\n",
    "Test(\"dataframes/en/English1.csv\", \"dataframes/da/Danish1.csv\", False)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
