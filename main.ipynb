{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies (First, look at README)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import import_ipynb\n",
    "import tampering_strategies as ts\n",
    "import data_sampling as ds\n",
    "from google import genai\n",
    "import Levenshtein\n",
    "from random_word import RandomWords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"English\"\n",
    "\n",
    "# with open(\"../en.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(200):\n",
    "#         lines = [next(f).strip() for _ in range(10000)]\n",
    "#         df = pandas.DataFrame({\"Text\": lines})\n",
    "#         output_file = f\"dataframes/en/{name}{i+1}.csv\"\n",
    "#         df.to_csv(output_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# name = \"Danish\"\n",
    "\n",
    "# with open(\"../da.da\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(200):\n",
    "#         lines = [next(f).strip() for _ in range(10000)]\n",
    "#         df = pandas.DataFrame({\"Text\": lines})\n",
    "#         output_file = f\"dataframes/da/{name}{i+1}.csv\"\n",
    "#         df.to_csv(output_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "\n",
    "# danishData = pandas.read_fwf(\"test.txt\", header=None, names=[\"test_sentences\"])\n",
    "# englishData = pandas.read_fwf(\"test.txt\", header=None, names=[\"test_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "r = RandomWords()\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    reference_p = [word_tokenize(reference)]\n",
    "    candidate_p = word_tokenize(candidate)\n",
    "    score = sentence_bleu(reference_p, candidate_p)\n",
    "    return score\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "  reference = word_tokenize(reference)\n",
    "  candidate = word_tokenize(candidate)\n",
    "  meteor_score = round(meteor([candidate],reference), 4)\n",
    "  return meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Rouge test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = \"resumption of the session is now\"\n",
    "sen2 = \"resumption the of session is now\"\n",
    "res = calculate_rouge(sen1, sen2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform BLEU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_bleu(sen1, sen2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform METEOR test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_meteor(sen1, sen2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE/RESET OUTPUT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetTests():\n",
    "    name = \"output.csv\"\n",
    "    cols = [\"originalSentence\", \"danishSentence\", \"tamperingType\", \"tamperedSentence\", \"LLMScore\",\"Confidence_Lower\", \"Confidence_Upper\", \n",
    "            \"BLEU\", \"METEOR\", \"Rouge1 r\", \"Rouge1 p\", \"Rouge1 f\", \"Rouge2 r\", \"Rouge2 p\", \"Rouge2 f\", \"Rougel r\", \"Rougel p\", \"Rougel f\"]\n",
    "\n",
    "    with open(name, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=cols)\n",
    "        writer.writeheader()\n",
    "\n",
    "resetTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Test(enFile, daFile, outputSize, llm):\n",
    "    propLenIndeces = ds.properLengthIndeces(enFile)\n",
    "    negIndeces = ds.negationIndeces(enFile)\n",
    "    tampStrats = 10\n",
    "    sensPerTamp = int(outputSize / tampStrats)\n",
    "    indcs1 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs2 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs3 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs4 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs5 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs6 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs7 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs8 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcs9 = ds.getXRandomIndeces(sensPerTamp, propLenIndeces)\n",
    "    indcsNeg = ds.getXRandomIndeces(sensPerTamp, negIndeces)\n",
    "\n",
    "    def runTest(sen, tamp, tampType, da):\n",
    "        outputName = \"output.csv\"\n",
    "        columns = [\"originalSentence\", \"danishSentence\", \"tamperingType\", \"tamperedSentence\", \n",
    "                   \"LLMScore\", \"Confidence_Lower\", \"Confidence_Upper\", \"BLEU\", \"METEOR\", \"Rouge1 r\", \"Rouge1 p\", \"Rouge1 f\", \"Rouge2 r\", \"Rouge2 p\", \"Rouge2 f\", \n",
    "                   \"Rougel r\", \"Rougel p\", \"Rougel f\"]\n",
    "        tampSen = tamp(sen)\n",
    "        if tampSen[0]:\n",
    "            rouge = calculate_rouge(sen, tampSen[1])\n",
    "            res = {\"originalSentence\": sen, \"danishSentence\": da, \"tamperingType\": tampType, \"tamperedSentence\": tampSen[1], \n",
    "                   \"LLMScore\": -1, \"Confidence_Lower\": -1, \"Confidence_Upper\": -1, \"BLEU\": calculate_bleu(sen, tampSen[1]), \"METEOR\": calculate_meteor(sen, tampSen[1]), \n",
    "                   \"Rouge1 r\": rouge[0][\"rouge-1\"]['r'], \"Rouge1 p\": rouge[0][\"rouge-1\"]['p'], \"Rouge1 f\": rouge[0][\"rouge-1\"]['f'], \n",
    "                   \"Rouge2 r\": rouge[0][\"rouge-2\"]['r'], \"Rouge2 p\": rouge[0][\"rouge-2\"]['p'], \"Rouge2 f\": rouge[0][\"rouge-2\"]['f'], \n",
    "                   \"Rougel r\": rouge[0][\"rouge-l\"]['r'], \"Rougel p\": rouge[0][\"rouge-l\"]['p'], \"Rougel f\": rouge[0][\"rouge-l\"]['f']}\n",
    "            with open(outputName, mode=\"a\", newline=\"\", encoding='utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=columns)\n",
    "                \n",
    "                writer.writerow(res)\n",
    "\n",
    "\n",
    "    def testAllTamps(sen, da, indx):\n",
    "        if indx in indcs1: runTest(sen.lower(), ts.replaceWithRandomWord, \"Replace Token with Random Word\", da)\n",
    "        if indx in indcs2: runTest(sen.lower(), ts.swapWords, \"Swap words\", da.lower())\n",
    "        if indx in indcs3: runTest(sen.lower(), ts.removeToken, \"Remove Token\", da.lower())\n",
    "        if indx in indcs4: runTest(sen.lower(), ts.addRandomWord, \"Add random word\", da.lower())\n",
    "        if indx in indcs5: runTest(sen.lower(), ts.duplicateToken, \"Duplicate Token\", da.lower())\n",
    "        if indx in indcs6: runTest(sen.lower(), ts.replaceCharacter, \"Replace Character\", da.lower())\n",
    "        if indx in indcs7: runTest(sen.lower(), ts.duplicateCharacter, \"Duplicate character\", da.lower())\n",
    "        if indx in indcs8: runTest(sen.lower(), ts.removeCharacter, \"Remove Character\" ,da.lower())\n",
    "        if indx in indcs9: runTest(sen.lower(), ts.swapCharacters, \"Swap characters\", da.lower())\n",
    "        if indx in indcsNeg: runTest(sen.lower(), ts.negation, \"Negation\", da.lower())\n",
    "        \n",
    "\n",
    "    def testDataSets(en, da):\n",
    "        enSens = pandas.read_csv(en, encoding='utf-8')\n",
    "        daSens = pandas.read_csv(da, encoding='utf-8')\n",
    "        for index, row in enSens.iterrows():\n",
    "            original = row[\"Text\"]\n",
    "            transRef = daSens.iloc[index][\"Text\"]\n",
    "            testAllTamps(original, transRef, index)\n",
    "\n",
    "    \n",
    "    def makePrompt(dan, en, tamp):\n",
    "        res = f\"\"\"Here are two sentences perfectly translated from Danish into English:\n",
    "\n",
    "            Sentence 1 (Danish): {dan}\n",
    "            Sentence 2 (English): {en}\n",
    "\n",
    "            Now, I will give you a third sentence, which is a tampered English translation: Sentence 3: {tamp}. \n",
    "\n",
    "            Compare the semantic similarity between Sentence 2 and Sentence 3 on a scale from 0 (totally different semantically) to 1 (totally identical semantically). \n",
    "            Provide a similarity score and confidence intervals as three values, each with two decimal places, in this exact plain text format:\n",
    "\n",
    "            Similarity Score: X.XX\n",
    "            Confidence Lower Bound: X.XX\n",
    "            Confidence Upper Bound: X.XX\n",
    "\n",
    "            All three values are required.\n",
    "            \"\"\"\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def addLLMScore(res):\n",
    "        client = genai.Client(api_key=\"AIzaSyBEx6cNuDD9XgrV0oO10TZ7ZQzzddCr_r8\")\n",
    "        data = pandas.read_csv(res, encoding='utf-8')\n",
    "        for index, row in data.iterrows():\n",
    "            english = row[\"originalSentence\"]\n",
    "            danish = row[\"danishSentence\"]\n",
    "            tamp = row[\"tamperedSentence\"]\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\", contents=makePrompt(danish, english, tamp)\n",
    "                )\n",
    "            \n",
    "            print(f\"Response at index {index}: {response.text}\")\n",
    "\n",
    "            score_match = re.search(r\"Similarity Score: (\\d\\.\\d{2})\", response.text)\n",
    "            lower_match = re.search(r\"Confidence Lower Bound: (\\d\\.\\d{2})\", response.text)\n",
    "            upper_match = re.search(r\"Confidence Upper Bound: (\\d\\.\\d{2})\", response.text)\n",
    "\n",
    "            if score_match:\n",
    "                similarity_score = score_match.group(1)  \n",
    "            if lower_match:\n",
    "                confidence_lower = lower_match.group(1)\n",
    "            if upper_match:\n",
    "                confidence_upper = upper_match.group(1) \n",
    "\n",
    "            data.loc[index, \"LLMScore\"] = similarity_score\n",
    "            data.loc[index, \"Confidence_Lower\"] = confidence_lower\n",
    "            data.loc[index, \"Confidence_Upper\"] = confidence_upper\n",
    "            \n",
    "            time.sleep(4)\n",
    "\n",
    "        data.to_csv(\"output.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "    testDataSets(enFile, daFile)\n",
    "    if llm: addLLMScore(\"output.csv\")\n",
    "\n",
    "# Test(\"en_test_data.csv\", \"da_test_data.csv\", 20, True)\n",
    "# Test(\"en_test_data.csv\", \"da_test_data.csv\", 20, False)\n",
    "Test(\"dataframes/en/English1.csv\", \"dataframes/da/Danish1.csv\", 10, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
